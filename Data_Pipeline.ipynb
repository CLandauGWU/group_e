{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the BBL list\n",
    "BBL12_17CSV = ['https://opendata.arcgis.com/datasets/82ab09c9541b4eb8ba4b537e131998ce_22.csv', 'https://opendata.arcgis.com/datasets/4c4d6b4defdf4561b737a594b6f2b0dd_23.csv',   'https://opendata.arcgis.com/datasets/d7aa6d3a3fdc42c4b354b9e90da443b7_1.csv',     'https://opendata.arcgis.com/datasets/a8434614d90e416b80fbdfe2cb2901d8_2.csv', 'https://opendata.arcgis.com/datasets/714d5f8b06914b8596b34b181439e702_36.csv',     'https://opendata.arcgis.com/datasets/c4368a66ce65455595a211d530facc54_3.csv',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addr_shape(shapetype):\n",
    "    \n",
    "    #Process user-defined shapefile\n",
    "    assert shapetype == 'census' or 'ward' or 'anc' #Currently only supports these\n",
    "    \n",
    "    crs='EPSG:4326' #Convenience assignment of crs throughout\n",
    "    \n",
    "    if shapetype == 'census':\n",
    "        shp_fl = down_extract_zip(\n",
    "            'https://opendata.arcgis.com/datasets/6969dd63c5cb4d6aa32f15effb8311f3_8.zip'\n",
    "        ) #Download the zip file and extract it, then assign the shapefile path\n",
    "        shp_census = gpd.read_file(shp_fl, crs=crs)\n",
    "        shp_df     = gpd.GeoDataFrame(shp_census,\n",
    "                                  crs=crs,\n",
    "                                  geometry=shp_census['geometry']\n",
    "                                 )\n",
    "\n",
    "    \n",
    "    if shapetype == 'ward':\n",
    "        shp_fl = down_extract_zip(\n",
    "            'https://opendata.arcgis.com/datasets/0ef47379cbae44e88267c01eaec2ff6e_31.zip'\n",
    "        ) #Download the zip file and extract it, then assign the shapefile path\n",
    "        shp_ward   = gpd.read_file(shp_fl, crs=crs)\n",
    "        shp_df     = gpd.GeoDataFrame(shp_ward,\n",
    "                                  crs=crs,\n",
    "                                  geometry=shp_ward['geometry']\n",
    "                                 )\n",
    "\n",
    "    if shapetype == 'anc':\n",
    "        shp_fl = down_extract_zip(\n",
    "            'https://opendata.arcgis.com/datasets/fcfbf29074e549d8aff9b9c708179291_1.zip'\n",
    "        ) #Download the zip file and extract it, then assign the shapefile path\n",
    "        shp_anc    = gpd.read_file(shp_fl, crs=crs)\n",
    "        shp_df     = gpd.GeoDataFrame(shp_anc,\n",
    "                                  crs=crs,\n",
    "                                  geometry=shp_anc['geometry']\n",
    "                                 )\n",
    "        \n",
    "    \n",
    "    adr_df   = pd.read_csv('https://opendata.arcgis.com/datasets/aa514416aaf74fdc94748f1e56e7cc8a_0.csv',\n",
    "                     encoding = 'utf-8', low_memory= False)\n",
    "\n",
    "    return [adr_df, shp_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def down_extract_zip(url):\n",
    "    #Downloads and unzips a DCopendata shape file\n",
    "    #and returns the filepath of the shape file\n",
    "    \n",
    "    #Usage: must have file named 'data' in cwd.\n",
    "    #Then: flname = down_extract_zip(url_of_zipfile)\n",
    "    #flname is now the path of the shpfile.\n",
    "    \n",
    "    local_filename = str('./data/' + url.split('/')[-1])\n",
    "        \n",
    "    r = requests.get(url)\n",
    "    assert r.status_code == 200 #Check connection\n",
    "    with open(local_filename, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    zip_fld = local_filename[:-4]\n",
    "    if not os.path.exists(zip_fld):\n",
    "        os.makedirs(zip_fld)\n",
    "        with zipfile.ZipFile(local_filename, \"r\") as zip_pl:\n",
    "            zip_pl.extractall(zip_fld)\n",
    "    fld_arr = os.listdir(zip_fld) #get array of unzipped files\n",
    "    for unzippedfile in fld_arr:  #find shapefile in unzipped files\n",
    "        if str(unzippedfile[-3:]) == 'shp':\n",
    "            flname = zip_fld + '/' + unzippedfile\n",
    "    if not flname: \n",
    "        print('Shapefile not found!')\n",
    "    return flname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def joinOurShapes(dex=None, toJoin=None, transf=None):\n",
    "    assert isinstance(toJoin, pd.DataFrame) #Must join a dataframe!\n",
    "    \n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80250, 35)\n",
      "(43557, 35)\n"
     ]
    }
   ],
   "source": [
    "def data_pipeline(shapetype, bbl_links, supplement=None,\n",
    "                 dex=None, ts_lst_range=None):\n",
    "    #A pipeline for group_e dataframe operations\n",
    "    \n",
    "    #Test inputs\n",
    "    if supplement:\n",
    "        assert isinstance(supplement, dict)\n",
    "    assert isinstance(bbl_links, list)\n",
    "    if ts_lst_range:\n",
    "        assert isinstance(ts_lst_range, list)\n",
    "        assert len(ts_lst_range) == 2 #Must be list of format [start-yr, end-yr]\n",
    "        \n",
    "\n",
    "    \n",
    "    #We'll need our addresspoints and our shapefile\n",
    "    #for the time_unit_of_analysis\n",
    "    if not dex:\n",
    "        dex = addr_shape(shapetype)\n",
    "    \n",
    "    #We need a list of time_unit_of_analysis\n",
    "    if ts_lst_range:\n",
    "        ts_lst = [x+(i/100) for i in range(1,13,1) for x in range(1980, 2025)]\n",
    "        ts_lst = [x for x in ts_lst if \n",
    "                  x >= ts_lst_range[0] and x <= ts_lst_range[1]]\n",
    "        ts_lst = sorted(ts_lst)\n",
    "    if not ts_lst_range:\n",
    "        ts_lst = [x+(i/100) for i in range(1,13,1) for x in range(2012, 2017)]\n",
    "        ts_lst = sorted(ts_lst)\n",
    "    \n",
    "    #Now we need to stack our BBL data\n",
    "    \n",
    "    #Begin by forming an empty DF\n",
    "    bbl_df = pd.DataFrame()\n",
    "    for i in bbl_links:\n",
    "        bbl = pd.read_csv(i, encoding='utf-8', low_memory=False)\n",
    "        col_len = len(bbl.columns)\n",
    "        bbl_df = bbl_df.append(bbl)\n",
    "        if len(bbl.columns) != col_len:\n",
    "            print('Column Mismatch!')\n",
    "        del bbl\n",
    "    bbl_df.LICENSE_START_DATE = pd.to_datetime(bbl_df.LICENSE_START_DATE)\n",
    "\n",
    "    bbl_df.sort_values('LICENSE_START_DATE')\n",
    "\n",
    "    bbl_df['month'] = 0\n",
    "    \n",
    "    bbl_df['month'] = bbl_df['LICENSE_START_DATE'].dt.year + (\n",
    "        bbl_df['LICENSE_START_DATE'].dt.month/100\n",
    "    )\n",
    "    \n",
    "    bbl_df = bbl_df.dropna(subset=['month'])\n",
    "    bbl_df = bbl_df.set_index(['MARADDRESSREPOSITORYID','month'])\n",
    "    bbl_df = bbl_df.sort_index(ascending=True)\n",
    "    bbl_df.reset_index(inplace=True)\n",
    "    print(bbl_df.shape)\n",
    "    bbl_df = bbl_df[bbl_df['LICENSESTATUS']== 'ACTIVE']\n",
    "    bbl_df = bbl_df.dropna('LICENSESTATUS')\n",
    "    print(bbl_df.shape)\n",
    "    \n",
    "    #Now that we have the BBL data, let's create our flag and points data.\n",
    "    addr_df = dex[0]\n",
    "    addr_df['geometry'] = [Point(xy) for xy in zip(addr_df.LONGITUDE.apply(float), addr_df.LATITUDE.apply(float))]\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "dex = addr_shape('anc')\n",
    "data_pipeline('anc', BBL12_17CSV, supplement=None, dex=dex, ts_lst_range=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['flag'][dffunctuionschei] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jdwqad'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teststr = 'jdwqadl;w'\n",
    "teststr[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
