{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as plt\n",
    "\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "from downloading_funcs import addr_shape, down_extract_zip\n",
    "from supp_funcs import *\n",
    "import lnks\n",
    "\n",
    "import warnings #DANGER: I triggered a ton of warnings.\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the BBL list\n",
    "BBL12_17CSV = ['https://hub.arcgis.com/datasets/82ab09c9541b4eb8ba4b537e131998ce_22.csv', 'https://hub.arcgis.com/datasets/4c4d6b4defdf4561b737a594b6f2b0dd_23.csv',   'https://hub.arcgis.com/datasets/d7aa6d3a3fdc42c4b354b9e90da443b7_1.csv',     'https://hub.arcgis.com/datasets/a8434614d90e416b80fbdfe2cb2901d8_2.csv', 'https://hub.arcgis.com/datasets/714d5f8b06914b8596b34b181439e702_36.csv',     'https://hub.arcgis.com/datasets/c4368a66ce65455595a211d530facc54_3.csv',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_pipeline(shapetype, bbl_links, supplement=None,\n",
    "                 dex=None, ts_lst_range=None):\n",
    "    #A pipeline for group_e dataframe operations\n",
    "    \n",
    "    \n",
    "    #Test inputs --------------------------------------------------------------\n",
    "    if supplement:\n",
    "        assert isinstance(supplement, list)\n",
    "    assert isinstance(bbl_links, list)\n",
    "    if ts_lst_range:\n",
    "        assert isinstance(ts_lst_range, list)\n",
    "        assert len(ts_lst_range) == 2 #Must be list of format [start-yr, end-yr]\n",
    "    \n",
    "    #We'll need our addresspoints and our shapefile\n",
    "    if not dex:\n",
    "        dex = addr_shape(shapetype)\n",
    "    \n",
    "    #We need a list of time_unit_of_analysis\n",
    "    if ts_lst_range:\n",
    "        ts_lst = [x+(i/100) for i in range(1,13,1) for x in range(1980, 2025)]\n",
    "        ts_lst = [x for x in ts_lst if \n",
    "                  x >= ts_lst_range[0] and x <= ts_lst_range[1]]\n",
    "        ts_lst = sorted(ts_lst)\n",
    "    if not ts_lst_range:\n",
    "        ts_lst = [x+(i/100) for i in range(1,13,1) for x in range(2012, 2017)]\n",
    "        ts_lst = sorted(ts_lst)\n",
    "    \n",
    "    #Now we need to stack our BBL data ----------------------------------------\n",
    "    \n",
    "    #Begin by forming an empty DF \n",
    "    bbl_df = pd.DataFrame()\n",
    "    for i in list(range(2012, 2018)):\n",
    "        bblpth = './data/bbls/Basic_Business_License_in_'+str(i)+'.csv' #Messy hack\n",
    "        #TODO: generalize bblpth above\n",
    "        bbl = pd.read_csv(bblpth, low_memory=False)\n",
    "        col_len = len(bbl.columns)\n",
    "        bbl_df = bbl_df.append(bbl)\n",
    "        if len(bbl.columns) != col_len:\n",
    "            print('Column Mismatch!')\n",
    "        del bbl\n",
    "        \n",
    "    bbl_df.LICENSE_START_DATE      = pd.to_datetime(\n",
    "        bbl_df.LICENSE_START_DATE)\n",
    "    \n",
    "    bbl_df.LICENSE_EXPIRATION_DATE = pd.to_datetime(\n",
    "        bbl_df.LICENSE_EXPIRATION_DATE)\n",
    "    \n",
    "    bbl_df.LICENSE_ISSUE_DATE      = pd.to_datetime(\n",
    "        bbl_df.LICENSE_ISSUE_DATE)\n",
    "\n",
    "    \n",
    "    bbl_df.sort_values('LICENSE_START_DATE')\n",
    "        \n",
    "    #Set up our time unit of analysis\n",
    "    bbl_df['month']      = 0\n",
    "    bbl_df['endMonth']   = 0\n",
    "    bbl_df['issueMonth'] = 0\n",
    "    \n",
    "    bbl_df['month'] = bbl_df['LICENSE_START_DATE'].dt.year + (\n",
    "        bbl_df['LICENSE_START_DATE'].dt.month/100\n",
    "    )\n",
    "    bbl_df['endMonth'] = bbl_df['LICENSE_EXPIRATION_DATE'].dt.year + (\n",
    "        bbl_df['LICENSE_EXPIRATION_DATE'].dt.month/100\n",
    "    )\n",
    "    bbl_df['issueMonth'] = bbl_df['LICENSE_ISSUE_DATE'].dt.year + (\n",
    "        bbl_df['LICENSE_ISSUE_DATE'].dt.month/100\n",
    "    )\n",
    "    bbl_df.endMonth.fillna(max(ts_lst))\n",
    "    bbl_df['endMonth'][bbl_df['endMonth'] > max(ts_lst)] = max(ts_lst)\n",
    "       \n",
    "    #Sort on month\n",
    "    bbl_df = bbl_df.dropna(subset=['month'])\n",
    "    bbl_df = bbl_df.set_index(['MARADDRESSREPOSITORYID','month'])\n",
    "    bbl_df = bbl_df.sort_index(ascending=True)\n",
    "    bbl_df.reset_index(inplace=True)\n",
    "    \n",
    "        \n",
    "    bbl_df = bbl_df[bbl_df['MARADDRESSREPOSITORYID'] >= 0]\n",
    "        \n",
    "    bbl_df = bbl_df.dropna(subset=['LICENSESTATUS', 'issueMonth', 'endMonth',\n",
    "                                   'MARADDRESSREPOSITORYID','month', \n",
    "                                   'LONGITUDE', 'LATITUDE'\n",
    "                                  ])\n",
    "    \n",
    "    #Now that we have the BBL data, let's create our flag and points data -----\n",
    "    \n",
    "    #This is the addresspoints, passed from the dex param\n",
    "    addr_df = dex[0]\n",
    "    \n",
    "    #Zip the latlongs\n",
    "    addr_df['geometry'] = [\n",
    "        Point(xy) for xy in zip(\n",
    "            addr_df.LONGITUDE.apply(float), addr_df.LATITUDE.apply(float)\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    addr_df['Points']   = addr_df['geometry'] #Duplicate, so raw retains points\n",
    "    \n",
    "    addr_df['dummy_counter'] = 1 #Always one, always dropped before export\n",
    "    \n",
    "    crs='EPSG:4326' #Convenience assignment of crs\n",
    "    \n",
    "    #Now we're stacking for each month ----------------------------------------\n",
    "    \n",
    "    \n",
    "    out_gdf = pd.DataFrame() #Empty storage df\n",
    "    for i in ts_lst: #iterate through the list of months\n",
    "        print('Month '+ str(i))\n",
    "        strmfile_pth = str(\n",
    "                './data/strm_file/' + str(i) +'_' + shapetype + '.csv')\n",
    "        if os.path.exists(strmfile_pth):\n",
    "            print('Skipping, ' + str(i) + ' stream file path already exists:')\n",
    "            print(strmfile_pth)\n",
    "            continue\n",
    "\n",
    "        #dex[1] is the designated shapefile passed from the dex param, \n",
    "        #and should match the shapetype defined in that param\n",
    "        \n",
    "        #Copy of the dex[1] shapefile\n",
    "        shp_gdf = dex[1]\n",
    "        \n",
    "        #Active BBL in month i\n",
    "        bbl_df['inRange'] = 0\n",
    "        bbl_df['inRange'][(bbl_df.endMonth > i) & (bbl_df.month <= i)] = 1\n",
    "        \n",
    "        #Issued BBL in month i\n",
    "        bbl_df['isuFlag'] = 0\n",
    "        bbl_df['isuFlag'][bbl_df.issueMonth == i] = 1\n",
    "        \n",
    "        #Merge BBL and MAR datasets -------------------------------------------\n",
    "        addr    = pd.merge(addr_df, bbl_df, how='left', \n",
    "                        left_on='ADDRESS_ID', right_on='MARADDRESSREPOSITORYID')\n",
    "        addr    = gpd.GeoDataFrame(addr, crs=crs, geometry=addr.geometry)\n",
    "        \n",
    "        shp_gdf.crs = addr.crs\n",
    "        \n",
    "        raw     = gpd.sjoin(shp_gdf, addr, how='left', op='intersects')\n",
    "        \n",
    "        #A simple percent of buildings with active flags per shape,\n",
    "        #and call it a 'utilization index'\n",
    "        numer = raw.groupby('NAME').sum()\n",
    "        numer = numer.inRange\n",
    "        denom = raw.groupby('NAME').sum()\n",
    "        denom = denom.dummy_counter\n",
    "        issue = raw.groupby('NAME').sum()\n",
    "        issue = issue.isuFlag\n",
    "        \n",
    "        flags = []\n",
    "        \n",
    "        utl_inx           = pd.DataFrame(numer/denom)\n",
    "        \n",
    "        utl_inx.columns   = [\n",
    "            'Util_Indx_BBL'\n",
    "        ]\n",
    "        flags.append(utl_inx)\n",
    "        \n",
    "        #This is number of buildings with an active BBL in month i\n",
    "        bbl_count         = pd.DataFrame(numer)\n",
    "        \n",
    "        bbl_count.columns = [\n",
    "            'countBBL'\n",
    "        ]\n",
    "        flags.append(bbl_count)\n",
    "        \n",
    "        #This is number of buildings that were issued a BBL in month i\n",
    "        isu_count         = pd.DataFrame(issue)\n",
    "        isu_count.columns = [\n",
    "            'countIssued'\n",
    "        ]\n",
    "        flags.append(isu_count)\n",
    "        \n",
    "        for flag in flags:\n",
    "            flag.crs = shp_gdf.crs\n",
    "\n",
    "            shp_gdf = shp_gdf.merge(flag,\n",
    "                                    how=\"left\", left_on='NAME', right_index=True)\n",
    "        shp_gdf['month'] = i\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Head will be the list of retained columns\n",
    "        head = ['NAME', 'Util_Indx_BBL',\n",
    "               'countBBL', 'countIssued',\n",
    "               'month', 'geometry']\n",
    "        shp_gdf = shp_gdf[head]\n",
    "        \n",
    "        print('Merging...')\n",
    "        if supplement: #this is where your code will be fed into the pipeline.\n",
    "            \n",
    "            #To include time unit of analysis, pass 'i=i' as the last\n",
    "            #item in your args list over on lnks.py, and the for-loop\n",
    "            #will catch that. Else, it will pass your last item as an arg.\n",
    "            \n",
    "            #Ping CDL if you need to pass a func with more args and we\n",
    "            #can extend this.\n",
    "            \n",
    "            for supp_func in supplement:\n",
    "                if len(supp_func) == 2:\n",
    "                    if supp_func[1] == 'i=i':\n",
    "                        shp_gdf = supp_func[0](shp_gdf, raw, i=i)\n",
    "                    if supp_func[1] != 'i=i':\n",
    "                        shp_gdf = supp_func[0](shp_gdf, raw, supp_func[1])\n",
    "                \n",
    "                if len(supp_func) == 3:\n",
    "                    if supp_func[2] == 'i=i':\n",
    "                        shp_gdf = supp_func[0](shp_gdf, raw, supp_func[1], i=i)\n",
    "                    if supp_func[2] != 'i=i':\n",
    "                        shp_gdf = supp_func[0](shp_gdf, raw, supp_func[1],\n",
    "                                              supp_func[2])\n",
    "                if len(supp_func) == 4:\n",
    "                    if supp_func[3] == 'i=i':\n",
    "                        shp_gdf = supp_func[0](shp_gdf, raw, supp_func[1],\n",
    "                                              supp_func[2], i=i)\n",
    "                    if supp_func[3] != 'i=i':\n",
    "                        shp_gdf = supp_func[0](shp_gdf, raw, supp_func[1],\n",
    "                                              supp_func[2], supp_func[3])\n",
    "                print(str(supp_func[0]) + ' is done.')\n",
    "        \n",
    "        \n",
    "        if not os.path.exists(strmfile_pth):\n",
    "            shp_gdf = shp_gdf.drop('geometry', axis=1)\n",
    "            \n",
    "            #Save, also verify re-read works\n",
    "            shp_gdf.to_csv(strmfile_pth, encoding='utf-8', index=False)\n",
    "            shp_gdf = pd.read_csv(strmfile_pth, encoding='utf-8', \n",
    "                                  engine='python')\n",
    "        del shp_gdf, addr, utl_inx, numer, denom, issue, raw #Save me some memory please!\n",
    "        #if i != 2016.12:\n",
    "        #    del raw\n",
    "        print('Merged month:', i)\n",
    "        print()\n",
    "        \n",
    "    #Done iterating through months here....\n",
    "    pth = './data/strm_file/' #path of the streamfiles\n",
    "    for file in os.listdir(pth):\n",
    "        try:\n",
    "            filepth = str(os.path.join(pth, file))\n",
    "            print([os.path.getsize(filepth), filepth])\n",
    "            fl = pd.read_csv(filepth, encoding='utf-8', engine='python') #read the stream file\n",
    "            out_gdf = out_gdf.append(fl) #This does the stacking\n",
    "            del fl\n",
    "        except IsADirectoryError:\n",
    "            continue\n",
    "    out_gdf.to_csv('./data/' + shapetype + '_out.csv') #Save\n",
    "    #shutil.rmtree('./data/strm_file/')\n",
    "    \n",
    "    print('Done!')\n",
    "    return [bbl_df, addr_df, out_gdf] #Remove this later, for testing now\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dex = addr_shape('anc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2012.01\n",
      "Skipping, 2012.01 stream file path already exists:\n",
      "./data/strm_file/2012.01_anc.csv\n",
      "Month 2012.02\n",
      "Skipping, 2012.02 stream file path already exists:\n",
      "./data/strm_file/2012.02_anc.csv\n",
      "Month 2012.03\n",
      "Skipping, 2012.03 stream file path already exists:\n",
      "./data/strm_file/2012.03_anc.csv\n",
      "Month 2012.04\n",
      "Skipping, 2012.04 stream file path already exists:\n",
      "./data/strm_file/2012.04_anc.csv\n",
      "Month 2012.05\n",
      "Skipping, 2012.05 stream file path already exists:\n",
      "./data/strm_file/2012.05_anc.csv\n",
      "Month 2012.06\n",
      "Skipping, 2012.06 stream file path already exists:\n",
      "./data/strm_file/2012.06_anc.csv\n",
      "Month 2012.07\n",
      "Skipping, 2012.07 stream file path already exists:\n",
      "./data/strm_file/2012.07_anc.csv\n",
      "Month 2012.08\n",
      "Skipping, 2012.08 stream file path already exists:\n",
      "./data/strm_file/2012.08_anc.csv\n",
      "Month 2012.09\n",
      "Skipping, 2012.09 stream file path already exists:\n",
      "./data/strm_file/2012.09_anc.csv\n",
      "Month 2012.1\n",
      "Skipping, 2012.1 stream file path already exists:\n",
      "./data/strm_file/2012.1_anc.csv\n",
      "Month 2012.11\n",
      "Skipping, 2012.11 stream file path already exists:\n",
      "./data/strm_file/2012.11_anc.csv\n",
      "Month 2012.12\n",
      "Skipping, 2012.12 stream file path already exists:\n",
      "./data/strm_file/2012.12_anc.csv\n",
      "Month 2013.01\n",
      "Skipping, 2013.01 stream file path already exists:\n",
      "./data/strm_file/2013.01_anc.csv\n",
      "Month 2013.02\n",
      "Skipping, 2013.02 stream file path already exists:\n",
      "./data/strm_file/2013.02_anc.csv\n",
      "Month 2013.03\n",
      "Skipping, 2013.03 stream file path already exists:\n",
      "./data/strm_file/2013.03_anc.csv\n",
      "Month 2013.04\n",
      "Skipping, 2013.04 stream file path already exists:\n",
      "./data/strm_file/2013.04_anc.csv\n",
      "Month 2013.05\n",
      "Skipping, 2013.05 stream file path already exists:\n",
      "./data/strm_file/2013.05_anc.csv\n",
      "Month 2013.06\n",
      "Skipping, 2013.06 stream file path already exists:\n",
      "./data/strm_file/2013.06_anc.csv\n",
      "Month 2013.07\n",
      "Skipping, 2013.07 stream file path already exists:\n",
      "./data/strm_file/2013.07_anc.csv\n",
      "Month 2013.08\n",
      "Skipping, 2013.08 stream file path already exists:\n",
      "./data/strm_file/2013.08_anc.csv\n",
      "Month 2013.09\n",
      "Skipping, 2013.09 stream file path already exists:\n",
      "./data/strm_file/2013.09_anc.csv\n",
      "Month 2013.1\n",
      "Skipping, 2013.1 stream file path already exists:\n",
      "./data/strm_file/2013.1_anc.csv\n",
      "Month 2013.11\n",
      "Skipping, 2013.11 stream file path already exists:\n",
      "./data/strm_file/2013.11_anc.csv\n",
      "Month 2013.12\n",
      "Skipping, 2013.12 stream file path already exists:\n",
      "./data/strm_file/2013.12_anc.csv\n",
      "Month 2014.01\n",
      "Skipping, 2014.01 stream file path already exists:\n",
      "./data/strm_file/2014.01_anc.csv\n",
      "Month 2014.02\n",
      "Skipping, 2014.02 stream file path already exists:\n",
      "./data/strm_file/2014.02_anc.csv\n",
      "Month 2014.03\n",
      "Skipping, 2014.03 stream file path already exists:\n",
      "./data/strm_file/2014.03_anc.csv\n",
      "Month 2014.04\n",
      "Skipping, 2014.04 stream file path already exists:\n",
      "./data/strm_file/2014.04_anc.csv\n",
      "Month 2014.05\n",
      "Skipping, 2014.05 stream file path already exists:\n",
      "./data/strm_file/2014.05_anc.csv\n",
      "Month 2014.06\n",
      "Skipping, 2014.06 stream file path already exists:\n",
      "./data/strm_file/2014.06_anc.csv\n",
      "Month 2014.07\n",
      "Merging...\n",
      "<function ITSPExtract at 0x7fee9bea9ea0> is done.\n",
      "<function clim_ingest at 0x7fee9bea9e18> is done.\n",
      "<function oecdGdpQs at 0x7fee9bea9d08> is done.\n"
     ]
    }
   ],
   "source": [
    "sets = data_pipeline('anc', BBL12_17CSV, supplement=lnks.supplm, dex=dex, ts_lst_range=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sets[2].columns #Our number of rows equals our number of shapes * number of months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
