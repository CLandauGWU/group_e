{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from downloading_funcs import addr_shape, down_extract_zip\n",
    "from supp_funcs import zoneConcentration, pointInZone\n",
    "import lnks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the BBL list\n",
    "BBL12_17CSV = ['https://opendata.arcgis.com/datasets/82ab09c9541b4eb8ba4b537e131998ce_22.csv', 'https://opendata.arcgis.com/datasets/4c4d6b4defdf4561b737a594b6f2b0dd_23.csv',   'https://opendata.arcgis.com/datasets/d7aa6d3a3fdc42c4b354b9e90da443b7_1.csv',     'https://opendata.arcgis.com/datasets/a8434614d90e416b80fbdfe2cb2901d8_2.csv', 'https://opendata.arcgis.com/datasets/714d5f8b06914b8596b34b181439e702_36.csv',     'https://opendata.arcgis.com/datasets/c4368a66ce65455595a211d530facc54_3.csv',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_pipeline(shapetype, bbl_links, supplement=None,\n",
    "                 dex=None, ts_lst_range=None):\n",
    "    #A pipeline for group_e dataframe operations\n",
    "    \n",
    "    \n",
    "    #Test inputs --------------------------------------------------------------\n",
    "    if supplement:\n",
    "        assert isinstance(supplement, list)\n",
    "    assert isinstance(bbl_links, list)\n",
    "    if ts_lst_range:\n",
    "        assert isinstance(ts_lst_range, list)\n",
    "        assert len(ts_lst_range) == 2 #Must be list of format [start-yr, end-yr]\n",
    "    \n",
    "    #We'll need our addresspoints and our shapefile\n",
    "    if not dex:\n",
    "        dex = addr_shape(shapetype)\n",
    "    \n",
    "    #We need a list of time_unit_of_analysis\n",
    "    if ts_lst_range:\n",
    "        ts_lst = [x+(i/100) for i in range(1,13,1) for x in range(1980, 2025)]\n",
    "        ts_lst = [x for x in ts_lst if \n",
    "                  x >= ts_lst_range[0] and x <= ts_lst_range[1]]\n",
    "        ts_lst = sorted(ts_lst)\n",
    "    if not ts_lst_range:\n",
    "        ts_lst = [x+(i/100) for i in range(1,13,1) for x in range(2012, 2017)]\n",
    "        ts_lst = sorted(ts_lst)\n",
    "    \n",
    "    #Now we need to stack our BBL data ----------------------------------------\n",
    "    \n",
    "    #Begin by forming an empty DF \n",
    "    bbl_df = pd.DataFrame()\n",
    "    for i in bbl_links:\n",
    "        bbl = pd.read_csv(i, encoding='utf-8', low_memory=False)\n",
    "        col_len = len(bbl.columns)\n",
    "        bbl_df = bbl_df.append(bbl)\n",
    "        if len(bbl.columns) != col_len:\n",
    "            print('Column Mismatch!')\n",
    "        del bbl\n",
    "        \n",
    "    bbl_df.LICENSE_START_DATE      = pd.to_datetime(\n",
    "        bbl_df.LICENSE_START_DATE)\n",
    "    \n",
    "    bbl_df.LICENSE_EXPIRATION_DATE = pd.to_datetime(\n",
    "        bbl_df.LICENSE_EXPIRATION_DATE)\n",
    "    \n",
    "    bbl_df.LICENSE_ISSUE_DATE      = pd.to_datetime(\n",
    "        bbl_df.LICENSE_ISSUE_DATE)\n",
    "\n",
    "    \n",
    "    bbl_df.sort_values('LICENSE_START_DATE')\n",
    "        \n",
    "    #Set up our time unit of analysis\n",
    "    bbl_df['month']      = 0\n",
    "    bbl_df['endMonth']   = 0\n",
    "    bbl_df['issueMonth'] = 0\n",
    "    \n",
    "    bbl_df['month'] = bbl_df['LICENSE_START_DATE'].dt.year + (\n",
    "        bbl_df['LICENSE_START_DATE'].dt.month/100\n",
    "    )\n",
    "    bbl_df['endMonth'] = bbl_df['LICENSE_EXPIRATION_DATE'].dt.year + (\n",
    "        bbl_df['LICENSE_EXPIRATION_DATE'].dt.month/100\n",
    "    )\n",
    "    bbl_df['issueMonth'] = bbl_df['LICENSE_ISSUE_DATE'].dt.year + (\n",
    "        bbl_df['LICENSE_ISSUE_DATE'].dt.month/100\n",
    "    )\n",
    "    bbl_df.endMonth.fillna(max(ts_lst))\n",
    "    bbl_df['endMonth'][bbl_df['endMonth'] > max(ts_lst)] = max(ts_lst)\n",
    "       \n",
    "    #Sort on month\n",
    "    bbl_df = bbl_df.dropna(subset=['month'])\n",
    "    bbl_df = bbl_df.set_index(['MARADDRESSREPOSITORYID','month'])\n",
    "    bbl_df = bbl_df.sort_index(ascending=True)\n",
    "    bbl_df.reset_index(inplace=True)\n",
    "    \n",
    "        \n",
    "    bbl_df = bbl_df[bbl_df['MARADDRESSREPOSITORYID'] >= 0]\n",
    "        \n",
    "    bbl_df = bbl_df.dropna(subset=['LICENSESTATUS', 'issueMonth', 'endMonth',\n",
    "                                   'MARADDRESSREPOSITORYID','month', \n",
    "                                   'LONGITUDE', 'LATITUDE'\n",
    "                                  ])\n",
    "    \n",
    "    #Now that we have the BBL data, let's create our flag and points data -----\n",
    "    \n",
    "    #This is the addresspoints, passed from the dex param\n",
    "    addr_df = dex[0]\n",
    "    \n",
    "    #Zip the latlongs\n",
    "    addr_df['geometry'] = [\n",
    "        Point(xy) for xy in zip(\n",
    "            addr_df.LONGITUDE.apply(float), addr_df.LATITUDE.apply(float)\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    addr_df['Points']   = addr_df['geometry'] #Duplicate, so raw retains points\n",
    "    \n",
    "    addr_df['dummy_counter'] = 1 #Always one, always dropped before export\n",
    "    \n",
    "    crs='EPSG:4326' #Convenience assignment of crs\n",
    "    \n",
    "    #Now we're stacking for each month ----------------------------------------\n",
    "    \n",
    "    out_gdf = pd.DataFrame() #Empty storage df\n",
    "\n",
    "    for i in ts_lst: #iterate through the list of months\n",
    "                \n",
    "        #dex[1] is the designated shapefile passed from the dex param, \n",
    "        #and should match the shapetype defined in that param\n",
    "        \n",
    "        #Copy of the dex[1] shapefile\n",
    "        shp_gdf = dex[1]\n",
    "        \n",
    "        #Active BBL in month i\n",
    "        bbl_df['inRange'] = 0\n",
    "        bbl_df['inRange'][(bbl_df.endMonth > i) & (bbl_df.month <= i)] = 1\n",
    "        \n",
    "        #Issued BBL in month i\n",
    "        bbl_df['isuFlag'] = 0\n",
    "        bbl_df['isuFlag'][bbl_df.issueMonth == i] = 1\n",
    "        \n",
    "        #Merge BBL and MAR datasets -------------------------------------------\n",
    "        addr    = pd.merge(addr_df, bbl_df, how='left', \n",
    "                        left_on='ADDRESS_ID', right_on='MARADDRESSREPOSITORYID')\n",
    "        addr    = gpd.GeoDataFrame(addr, crs=crs, geometry=addr.geometry)\n",
    "        \n",
    "        \n",
    "        addr.crs = shp_gdf.crs\n",
    "        raw     = gpd.sjoin(shp_gdf, addr, how='left', op='intersects')\n",
    "        \n",
    "        #A simple percent of buildings with active flags per shape,\n",
    "        #and call it a 'utilization index'\n",
    "        numer = raw.groupby('NAME').sum()\n",
    "        numer = numer.inRange\n",
    "        denom = raw.groupby('NAME').sum()\n",
    "        denom = denom.dummy_counter\n",
    "        issue = raw.groupby('NAME').sum()\n",
    "        issue = issue.isuFlag\n",
    "        \n",
    "        flags = []\n",
    "        \n",
    "        utl_inx           = pd.DataFrame(numer/denom)\n",
    "        \n",
    "        utl_inx.columns   = [\n",
    "            'Util_Indx_BBL'\n",
    "        ]\n",
    "        flags.append(utl_inx)\n",
    "        \n",
    "        #This is number of buildings with an active BBL in month i\n",
    "        bbl_count         = pd.DataFrame(numer)\n",
    "        \n",
    "        bbl_count.columns = [\n",
    "            'countBBL'\n",
    "        ]\n",
    "        flags.append(bbl_count)\n",
    "        \n",
    "        #This is number of buildings that were issued a BBL in month i\n",
    "        isu_count         = pd.DataFrame(issue)\n",
    "        isu_count.columns = [\n",
    "            'countIssued'\n",
    "        ]\n",
    "        flags.append(isu_count)\n",
    "        \n",
    "        for flag in flags:\n",
    "            flag.crs = shp_gdf.crs\n",
    "\n",
    "            shp_gdf = shp_gdf.merge(flag,\n",
    "                                    how=\"left\", left_on='NAME', right_index=True)\n",
    "        shp_gdf['month'] = i\n",
    "        \n",
    "        #Head will be the list of retained columns\n",
    "        head = ['NAME', 'Util_Indx_BBL',\n",
    "               'countBBL', 'countIssued',\n",
    "               'month', 'geometry']\n",
    "        shp_gdf = shp_gdf[head]\n",
    "        \n",
    "        \n",
    "        if supplement: #this is where your code will be fed into the pipeline.\n",
    "            for supp_func in supplement:\n",
    "                if len(supp_func) == 2:\n",
    "                    shp_gdf = supp_func[0](shp_gdf, raw, supp_func[1])\n",
    "                if len(supp_func) == 3:\n",
    "                    shp_gdf = supp_func[0](shp_gdf, raw, supp_func[1],\n",
    "                                          supp_func[2])\n",
    "                if len(supp_func) == 4:\n",
    "                    shp_gdf = supp_func[0](shp_gdf, raw, supp_func[1],\n",
    "                                          supp_func[2], supp_func[3])\n",
    "                \n",
    "        \n",
    "        out_gdf = out_gdf.append(shp_gdf) #This does the stacking\n",
    "        print('Merged month:', i)\n",
    "        del shp_gdf, addr, utl_inx #Save me some memory please!\n",
    "    \n",
    "    #Can't have strings in our matrix\n",
    "    out_gdf = pd.get_dummies(out_gdf, columns=['NAME'])\n",
    "    out_gdf = out_gdf.drop('geometry', axis=1)\n",
    "    \n",
    "    out_gdf.to_csv('./data/' + shapetype + '_out.csv') #Save\n",
    "    \n",
    "    return [bbl_df, addr_df, out_gdf, raw] #Remove this later, for testing now\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dex = addr_shape('anc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metro_prox(shp_gdf, raw, bufr=None):\n",
    "    #Flag properties within distance \"bufr\" of metro stations\n",
    "    \n",
    "    if not bufr:\n",
    "        bufr = 1/250 #Hard to say what a good buffer is.\n",
    "    \n",
    "    assert isinstance(bufr, float) #buffer must be float!\n",
    "    \n",
    "    #Frame up the metro buffer shapes\n",
    "    metro = down_extract_zip(\n",
    "    'https://opendata.arcgis.com/datasets/54018b7f06b943f2af278bbe415df1de_52.zip'\n",
    "    )\n",
    "    metro          = gpd.read_file(metro, crs=shp_gdf.crs)\n",
    "    metro.geometry = metro.geometry.buffer(bufr)\n",
    "    metro['bymet'] = 1\n",
    "    metro.drop(['NAME'], axis=1, inplace=True)\n",
    "    \n",
    "    #Frame up the raw address points data\n",
    "    pointy         = raw[['NAME', 'Points', 'dummy_counter']]\n",
    "    pointy         = gpd.GeoDataFrame(pointy, crs=metro.crs, \n",
    "                                      geometry=pointy.Points)\n",
    "    pointy         = gpd.sjoin(pointy, metro, \n",
    "                               how='left', op='intersects')\n",
    "    \n",
    "    denom = pointy.groupby('NAME').sum()\n",
    "    denom = denom.dummy_counter\n",
    "    \n",
    "    numer = pointy.groupby('NAME').sum()\n",
    "    numer = numer.bymet\n",
    "    \n",
    "    pct_metro_coverage    = pd.DataFrame(numer/denom)\n",
    "        \n",
    "    pct_metro_coverage.columns   = [\n",
    "        'pct_metro_coverage'\n",
    "    ]\n",
    "    \n",
    "    pct_metro_coverage.fillna(0, inplace=True)\n",
    "    \n",
    "    pct_metro_coverage.crs = pointy.crs\n",
    "    shp_gdf = shp_gdf.merge(pct_metro_coverage,\n",
    "                        how=\"left\", left_on='NAME', right_index=True)\n",
    "    return shp_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6abd71661b7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'anc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBBL12_17CSV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupplement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlnks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupplm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_lst_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-5e581ca5397e>\u001b[0m in \u001b[0;36mdata_pipeline\u001b[0;34m(shapetype, bbl_links, supplement, dex, ts_lst_range)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msupp_func\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msupplement\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupp_func\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mshp_gdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupp_func\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshp_gdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupp_func\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupp_func\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                     shp_gdf = supp_func[0](shp_gdf, raw, supp_func[1],\n",
      "\u001b[0;32m~/group_e/supp_funcs.py\u001b[0m in \u001b[0;36mzoneConcentration\u001b[0;34m(shp_gdf, raw, pntLst, bufr)\u001b[0m\n\u001b[1;32m     37\u001b[0m                                       geometry=pointy.Points)\n\u001b[1;32m     38\u001b[0m     pointy         = gpd.sjoin(pointy, ftr, \n\u001b[0;32m---> 39\u001b[0;31m                                how='left', op='intersects')\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpointy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NAME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/geopandas/tools/sjoin.py\u001b[0m in \u001b[0;36msjoin\u001b[0;34m(left_df, right_df, how, op, lsuffix, rsuffix)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     idxmatch = (left_df.geometry.apply(lambda x: x.bounds)\n\u001b[0;32m---> 72\u001b[0;31m                 .apply(lambda x: list(tree_idx.intersection(x))))\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0midxmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midxmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer (pandas/_libs/lib.c:66645)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/geopandas/tools/sjoin.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     idxmatch = (left_df.geometry.apply(lambda x: x.bounds)\n\u001b[0;32m---> 72\u001b[0;31m                 .apply(lambda x: list(tree_idx.intersection(x))))\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0midxmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midxmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/rtree/index.py\u001b[0m in \u001b[0;36mintersection\u001b[0;34m(self, coordinates, objects)\u001b[0m\n\u001b[1;32m    463\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m                                     ctypes.byref(p_num_results))\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_num_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/rtree/core.py\u001b[0m in \u001b[0;36mcheck_return\u001b[0;34m(result, func, cargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;34m\"Error checking for Error calls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sets = data_pipeline('anc', BBL12_17CSV, supplement=lnks.supplm, dex=dex, ts_lst_range=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sets[2] #Our number of rows equals our number of shapes * number of months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
