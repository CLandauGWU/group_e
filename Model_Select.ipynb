{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fiona\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "from lnks import scl_cols\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings #DANGER: I triggered a ton of warnings.\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To start...\n",
    "\n",
    "We import the data output of our data pipeline. We reset the index, drop index columns, and lag the data.\n",
    "\n",
    "We explicitly print shape several times, making sure that we capture the magnitude of data lost from dropping NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data\n",
    "filestring = './data/anc_out.csv'\n",
    "df = pd.read_csv(filestring)\n",
    "df = df.sort_values(['month', 'NAME'])# , 'ANC'])\n",
    "df = df.reset_index(drop=True)\n",
    "len(df.NAME.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we examine the columns and lag the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "shiftmonths = 12\n",
    "shiftnum= (((len(df.NAME.unique()))*(shiftmonths)))\n",
    "\n",
    "#Also generate some lagged y data in the opposite direction.\n",
    "df['y']= df['countBBL'].shift(-shiftnum)\n",
    "df['countBBL_prev_month'] = df['countBBL'].shift((len(df.NAME.unique())))\n",
    "df['countBBL_prev_cycle'] = df['countBBL'].shift((shiftnum))\n",
    "df = df[shiftnum:-(shiftnum+(len(df.NAME.unique())))]\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell cleans out vestigial columns and drops/fills/expands to dummies for our NA and categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['NAME'])\n",
    "df = df.drop(['Unnamed: 0'], axis= 1)\n",
    "print(df.shape)\n",
    "df = df.astype('float')\n",
    "\n",
    "df = df.dropna()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start building our grid search inputs, beginning with the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Flexible adaptation of Dr. Braman's interactive gridsearch script\n",
    "#implementation. \n",
    "#TODO Clean up and streamline\n",
    "import sklearn\n",
    "from sklearn.neural_network import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.gaussian_process import *\n",
    "from sklearn.gaussian_process.kernels import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.discriminant_analysis import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import *\n",
    "import random\n",
    "\n",
    "#Frame up some separate DataFrames for scalar and stuff\n",
    "scl_data = data = df\n",
    "\n",
    "#Assign the split for holdout data.\n",
    "holdout_date = 2014.5\n",
    "\n",
    "data     = data.reset_index(drop=True)\n",
    "X = data.drop(['y'], axis=1)\n",
    "y = data['y']\n",
    "\n",
    "XH_train = data[data['month'] <= holdout_date]\n",
    "yH_train = XH_train['y']\n",
    "XH_train = XH_train.drop(['y'], axis=1)\n",
    "\n",
    "XH_test  = data[data['month'] >= holdout_date]\n",
    "yH_test  = XH_test['y']\n",
    "XH_test  = XH_test.drop(['y'], axis=1)\n",
    "\n",
    "ytr = sklearn.preprocessing.MinMaxScaler([0, 1]\n",
    "            ).fit(y)\n",
    "y = ytr.fit_transform(y)\n",
    "y = pd.DataFrame(y, columns=['y'])\n",
    "\n",
    "scl_data = scl_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(scl_data.month.max())\n",
    "print(scl_data.shape)\n",
    "scl_data = scl_data.dropna()\n",
    "print(scl_data.shape)\n",
    "sXH_train = scl_data[scl_data['month'] <= holdout_date]\n",
    "syH_train = sXH_train['y']\n",
    "sXH_train = sXH_train.drop(['y'], axis=1)\n",
    "\n",
    "sXH_test  = scl_data[scl_data['month'] >= holdout_date]\n",
    "syH_test  = sXH_test['y']\n",
    "sXH_test  = sXH_test.drop(['y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build scalers for the scl_data, other --------------------\n",
    "scale_data_splits = [scl_data, sXH_train,sXH_test, syH_train, syH_test]\n",
    "for scl_data in scale_data_splits:\n",
    "    scaler = sklearn.preprocessing.StandardScaler(\n",
    "                ).fit(scl_data)\n",
    "    minmaxer = sklearn.preprocessing.MinMaxScaler([0, 1]\n",
    "                ).fit(scl_data)\n",
    "\n",
    "    scl = scaler.transform(scl_data)\n",
    "    scl = minmaxer.transform(scl_data)\n",
    "    try:\n",
    "        scl_data = pd.DataFrame(scl, columns=scl_data.columns)\n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "        scl_data = pd.DataFrame(scl, columns=['y'])\n",
    "\n",
    "#scl_data[scl_data.columns\n",
    "#   ] = scaler.fit_transform(scl_data[scl_data.columns])\n",
    "\n",
    "#----------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's make sure our data came out of the scalers intact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sXH_train.shape)\n",
    "print(syH_train.shape)\n",
    "print(sXH_test.shape)\n",
    "print(syH_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sX = scl_data.drop(['y'], axis=1)\n",
    "sy = scl_data['y']\n",
    "\n",
    "\n",
    "\n",
    "assert np.all(np.isfinite(X))\n",
    "assert np.all(np.isfinite(y))\n",
    "assert not np.any(np.isnan(X))\n",
    "assert not np.any(np.isnan(y))\n",
    "\n",
    "assert np.all(np.isfinite(sX))\n",
    "assert np.all(np.isfinite(sy))\n",
    "assert not np.any(np.isnan(sX))\n",
    "assert not np.any(np.isnan(sy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell contains our a crude RNG, a list of regressors which benefit from scaled data, and hardcoded data used to generate our param_grid, et cetera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a short list of random states to insert into randomstate params.\n",
    "scrambler = []\n",
    "for scram in range(0, 10):\n",
    "    scrambler.append(random.randint(0, 10000))   \n",
    "print(scrambler)\n",
    "\n",
    "to_scale = ['SVR', 'LassoLars']\n",
    "\n",
    "names       = ['AdaBoostRegressor',\n",
    "             'RandomForestRegressor',\n",
    "             'SVR',\n",
    "             #'KNeighborsRegressor',\n",
    "             #'BaggingRegressor',\n",
    "             'GradientBoostingRegressor',\n",
    "             #'LinearRegression',\n",
    "             #'MLPRegressor',\n",
    "             #'SGDRegressor',\n",
    "             'LassoLars'         \n",
    "    \n",
    "]\n",
    "\n",
    "regressors = [AdaBoostRegressor(),\n",
    "              RandomForestRegressor(),\n",
    "              SVR(),\n",
    "              #KNeighborsRegressor(),\n",
    "              #BaggingRegressor(),\n",
    "              GradientBoostingRegressor(),\n",
    "              #LinearRegression(),\n",
    "              #MLPRegressor(),\n",
    "              #SGDRegressor(),\n",
    "              LassoLars()\n",
    "    \n",
    "]\n",
    "\n",
    "param_grids =[ \n",
    "    ['AdaBoostRegressor', dict(\n",
    "        n_estimators=[80, 60, 30],\n",
    "        learning_rate=[1, .5, .01],\n",
    "        loss=['linear', 'square', 'exponential'],\n",
    "        #random_state=scrambler[3:5]\n",
    "        \n",
    "    )],\n",
    "        \n",
    "    ['RandomForestRegressor', dict(\n",
    "        max_depth=[5, 10, 15],\n",
    "        criterion=['mse', 'mae'],\n",
    "        #random_state=scrambler[:2]\n",
    "    )],\n",
    "    ['SVR', dict( #Most params for SVR are turned off right now, too expensive\n",
    "        C=[1, .9],\n",
    "        #epsilon=[.1, .05],\n",
    "        #kernel=['rbf', 'sigmoid', 'poly']\n",
    "    )],\n",
    "    ['GradientBoostingRegressor', dict(\n",
    "        max_depth=[3, 6, 9, 12],\n",
    "        min_samples_split=[2, 4, 8],\n",
    "        presort=[False]\n",
    "    )],\n",
    "    ['LassoLars', dict(\n",
    "        alpha=[0.1, 1, .5, .75],\n",
    "        fit_path=[False]\n",
    "    )],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search:\n",
    "\n",
    "Here we implement an iterator that executes GridSearchCV and reports the best explained variance. The best_params attribute is then extracted, and used those on the whole training set, then predict on the holdout data.\n",
    "\n",
    "Testing indicates that for some models, the fit on our full dataset modestly outperforms the CV regularly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "outcomes = []\n",
    "\n",
    "for name, rgsr in zip(names, regressors):\n",
    "    \n",
    "    for item in param_grids:\n",
    "        if item[0]==name:\n",
    "            print(name + ':')\n",
    "            params= item[1]\n",
    "        \n",
    "    \n",
    "    cv = sklearn.model_selection.GridSearchCV(rgsr, param_grid=params,\n",
    "                                              verbose=True, n_jobs=12,\n",
    "                                              cv=3, pre_dispatch=\"2*n_jobs\")\n",
    "    \n",
    "    if name not in to_scale:\n",
    "        #X_train, y_train, X_test, y_test = sklearn.model_selection.train_test_split(X, y)\n",
    "        fitted = cv.fit(XH_train, yH_train)\n",
    "        score = cv.score(XH_test, yH_test)\n",
    "        print(score)\n",
    "\n",
    "        best = rgsr.set_params(**cv.best_params_)\n",
    "        bestfit= best.fit(XH_train, yH_train)\n",
    "        bestscore = best.score(XH_test, yH_test)\n",
    "    if name in to_scale:\n",
    "    #TODO: fix\n",
    "        #X_train, y_train, X_test, y_test = sklearn.model_selection.train_test_split(sX, sy)\n",
    "        fitted = cv.fit(sXH_train, syH_train)\n",
    "        score = cv.score(sXH_test, syH_test)\n",
    "        print(score)\n",
    "\n",
    "        best = rgsr.set_params(**cv.best_params_)\n",
    "        bestfit= best.fit(sXH_train, syH_train)\n",
    "        bestscore = best.score(sXH_test, syH_test)\n",
    "\n",
    "    print(name + \" R2 with best model, score:\")\n",
    "    print(bestscore)\n",
    "    \n",
    "    \n",
    "    outcomes.append((name, score, cv.cv_results_, cv.best_estimator_, \n",
    "                     cv.best_params_, bestscore))\n",
    "    \n",
    "for nm in range(0, len(outcomes)):\n",
    "    print()\n",
    "    print(outcomes[nm][0])\n",
    "    print(outcomes[nm][1])\n",
    "    print()\n",
    "    print('Best on real:')\n",
    "    print(outcomes[nm][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything below is exploratory analysis for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for jm in range(0, 5):\n",
    "    \n",
    "    print(outcomes[jm][0])\n",
    "    \n",
    "    print(outcomes[jm][1])\n",
    "    print(outcomes[jm][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best = AdaBoostRegressor(learning_rate=1, loss='square', n_estimators=60)\n",
    "bestfit= best.fit(XH_train, yH_train)\n",
    "bestscore = best.score(XH_test, yH_test)\n",
    "print(outcomes[0][0])\n",
    "print(bestscore)\n",
    "\n",
    "best = RandomForestRegressor(max_depth=10)\n",
    "bestfit= best.fit(XH_train, yH_train)\n",
    "bestscore = best.score(XH_test, yH_test)\n",
    "print(outcomes[1][0])\n",
    "print(bestscore)\n",
    "\n",
    "best = SVR(max_depth=10)\n",
    "bestfit= best.fit(XH_train, yH_train)\n",
    "bestscore = best.score(XH_test, yH_test)\n",
    "print(outcomes[2][0])\n",
    "print(bestscore)\n",
    "\n",
    "\n",
    "best = GradientBoostingRegressor(max_depth=10)\n",
    "bestfit= best.fit(XH_train, yH_train)\n",
    "bestscore = best.score(XH_test, yH_test)\n",
    "print(outcomes[3][0])\n",
    "print(bestscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Xtrain = dat_xtrain.drop(['y'], axis=1)\n",
    "y16 = dat_ytrain['y']\n",
    "X15 = dat15.drop(['y'], axis=1)\n",
    "y15 = dat15['y']\n",
    "\n",
    "fitted    = outcomes[-2][3].fit(X15, y15)\n",
    "predicted = fitted.predict(X16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(predicted, columns=['predicted'])\n",
    "dat16 = dat16.reset_index()\n",
    "pred['y'] = dat16['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def flagger_ranges(pred):\n",
    "    pred['flag15'] = 0\n",
    "    pred['flag15'][pred['predicted'].between(pred['y']*0.85, pred['y']*1.15)\n",
    "                  ] = 1\n",
    "    pred['flag05'] = 0\n",
    "    pred['flag05'][pred['predicted'].between(pred['y']*0.85, pred['y']*1.15)\n",
    "                  ] = 1\n",
    "    pred['flag10'] = 0\n",
    "    pred['flag10'][pred['predicted'].between(pred['y']*0.85, pred['y']*1.15)\n",
    "                  ] = 1\n",
    "    pred['flag_others']= 0\n",
    "    pred['flag_others'][pred['flag05'] == 0] = 1\n",
    "    return pred\n",
    "pred = flagger_ranges(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
